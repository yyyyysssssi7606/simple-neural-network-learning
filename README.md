# 0基础无微积分前置知识笔记

## 一、Excel案例：我的第一个神经网络

我学习到了神经网络的前向传播和反向传播，现将相关代码和学习过程，以及相关笔记和感悟记录，希望通过记录所学知识，未来忘记的时候可以复习，也希望可以帮助更多对机器学习和神经网络感兴趣，但是又没有相关知识的朋友。

## 二、Excel案例：我的第一个神经网络

在我印象里，机器学习只有本科研究生数学水平的人才能学会的，我作为一个初中数学水平的人，一直对神经网络很感兴趣，但是我不会高等数学，因为我连高中都没有读过，就连高中数学都没有接触过，更别说高等数学，同时因为不了解相关前置知识，又不会英语，导致国内的视频看不懂，国外的英语教材也看不懂......

转折点在最近(2025年12月19日)，我在bilibili刷短视频的时候，刷到了[【【AI入门】【用Excel实现神经网络】】](https://b23.tv/yhO6559) 。因为看标题是使用Excel实现神经网络，所以我想看看是怎么回事，但是发现这也还是一个教程，带着心中对机器学习知识的渴望和无法学习的不甘，我想也没想的在评论区吐槽着零基础学不会，但是作者老师回复我，说这就是面向零基础的系列课程，而且还有Excel实现案例，我就抱着玩玩的心态，到github上下载了Excel案例 [zeroai](https://github.com/jiangsongyi0204/zeroai)，在电脑上运行。

原来神经网络是长这个样子的，这个神经网络是一个吃豆人的操作网络，九宫格里面，红色代表吃豆人的位置，绿色代表豆子的位置，豆子出现时，红色格子里的箭头代表要向哪个方向前进才能以最少步数吃到最多豆子，右侧的数据格子是权重数据，最右侧的是训练数据，一行代表一条数据，左侧的P1到P8代表输入的数据，右侧A1到A4代表这条数据的答案：

![Excel神经网络](./images/net01.png "Excel神经网络")

点击初始化神经网络，点击批量训练，然后点点点，点十几下，点的过程中发现线条变得粗细不一，然后再点测试按钮，就可以发现这个神经网络很神奇，通过某种计算，就可以让吃豆人吃到豆子：

![Excel神经网络2](./images/net02.png "Excel神经网络2")

## 三、揭开神经网络的面纱：前向传播与反向传播

这和我一开始我想的一样，就是众所周知的只有三层节点的网络，等我尝试复刻的时候，我发现这个如果只存在三层节点，根本无法做任何计算，因为计算是需要等式，而这三层节点，第一层节点存的是输入的参数，中间层的节点和最后一层节点存储的是计算结果，所以其中肯定存在别的计算参数，通过与输入参数进行计算才能得出结果，然后经过两轮计算才能得到最终的结果。

![Excel神经网络3](./images/net03.png "Excel神经网络3")

但是在右侧，我看到神经网络中还包含两个权重参数和两个偏置参数：

![Excel神经网络4](./images/net04.png "Excel神经网络4")

我突然想到，肯定是这两个参数参与计算，才能得出结果，所以实际上并非三层节点，而是3层节点+2层权重+2层偏置=7层节点，通过某种形式的计算，才得出来最终的结果，所以，刚刚那个红色和蓝色的线条，其实就是权重，所以线条会在训练后变粗：

![Excel神经网络5](./images/net05.png "Excel神经网络5")

上面是一个简化简化过的两层神经网络之间的计算过程示意图，图中的A代表输入层，W代表输出层，B代表偏置层，O代表输出层，作者老师说，通过以下公式计算出结果：
~~~text
输入层数据 * 权重1 + 偏置1 = 计算结果
input * weight1 + bias1 = temp_result

使用sigmoid激活函数函数计算此结果，得到隐藏层数据
sigmoid(temp_result) = hidden

隐藏层数据 * 权重2 + 偏置2 = 计算结果2
hidden * weight2 + bias2 = temp_result2

最后使用softmax激活函数将计算结果2的数据转化为总和为1(100%)的数，然后放在输出层
softmax(temp_result2) = output
~~~

> 这里的sigmoid函数和softmax函数属于激活函数，并没写出函数的内容，相当于占位符。
> 
> 作用是将线性计算的结果“挤压”或“归一化”到特定范围，使网络能学习非线性关系。
> 
> 可以简单理解为其中的参数经过某种计算以后，得到等号右边的结果，函数的具体内容在后面数学知识部分有讲解

当我尝试推导反向传播时，我发现它就像调整空调温度的过程，把`A`比作要调节的温度，`W`需要调整多少度，`B`至少要调节的度数，然后`temp_result`最终调整完成的温度。但是光调温度还不行，还要知道这个温度到底是冷还是热，这就需要心里面有一条判断温度的标准线，通过人体去体验这个温度是否合适，而这个判断的过程就是激活函数（Logistic）。

开空调的神经网络结果，会被softmax将最终的计算结果转化为概率。比如设置了五个心里答案，很热，有点热，正常，凉快，有点冷，通过转化概率，五个预期答案出现的概率总和是100%。比如很热12%，有点热38%，正常20%，凉快16%，有点冷14%，那么最大概率就是有点热。接下来就看哪个温度符合实际情况，如果温度高于或者低于自己的心理预期，就要往正确方向调整空调温度，神经网络也是走的这个过程。

因为神经网络无法一开始就判断空调温度的高低，就需要训练，输入当前空调温度，外界温度，以及其他环境因素，然后输出结果，根据正确答案和输出结果，调整空调温度，如果每次调1摄氏度，那调整的学习率就是1/(空调最高温 - 空调最低温)，让空调温度和自己觉得舒服的温度的差距变小，因为事先不知道最合理的温度是多少，所以需要小步快走，多次调整，直到达到自己想要的温度为止，这就是学习过程。

由多个计算过程的叠加，就形成了一个完整的针对多个指标的处理和判断的程序，比如同时输入温度，湿度，去判断体感温度是冷是热，空气是否干燥，是否潮湿等，这就是神经网络的原理，判断的方法有很多，这里使用`sigmoid`函数作为判断标准，通过有时候通过初次判断和再判断，会让结果更加的准确，所以会设计成多层的神经网络。

## 四、数学没那么可怕：微积分与链式法则的直观理解

开空调的时候，就是前向传播的过程，也是预测温度的过程，然后觉得冷，主动去调整空调温度，就是学习的过程，就是反向传播过程。

经过对神经网络的初步认识，就可以直接根据现有的公式进行计算，我也是在学习后得出结论，数学公式并没有那么高深莫测，只要知道符号含义，计算规则，就可以看懂这个数学公式，看不懂公式，是因为从来没有人告诉我公式里的符号是什么意思，如何去计算。

学习神经网络需要的几个知识点都可以跟生活中常见的事物联系起来，并不难理解，这些知识点，分别是微积分、链式法则、计算图、还有一点矩阵向量的知识，这些知识的名字看起来让人感到非常恐惧，但是好处就是我可以通过AI去帮我辅助理解和推理这一过程，AI能满足我打破砂锅问到底的好奇心。


### 4.1、公式和数学知识讲解

通过学习，我知道微积分描述的是一个事物变化过程中的量的拆分和合并，这个量不是固定的，是一直在变化的，像方程，但是方程有固定量，而微积分没固定量，没有固定量，意味着你可以带入任何数字到式子中，然后计算出结果，这些结果可以连接成一条线，这条线就是数学中的函数图。

#### 4.1.1 函数

一开始我不知道函数是什么，只知道别人说的定义一个`f(x)`，通过AI的解释之后，我终于知道了什么是函数，定义函数的目的是为了将复杂的公式简单化，让人方便的对多个复杂的公式进行计算，如果整个复杂的公式能写一本书，那函数就是这本书的目录，通过目录，可以知晓整个公式的计算目的，同时可以给这个式子命名，比如叫`sigmoid`函数，`sigmoid`函数已经被数学家定义好了，就像三角函数或者其他的什么函数一样，只需要理解其中符号的意思，然后拿来用就好，可以像搭乐高积木一样组合自己的函数，完成不同的操作：

![sigmoid激活函数方程](./images/sigmoid-function.png "sigmoid激活函数方程")

比如上面公式中的`f(x)`表示我要定义一个函数，这个函数需要根据x求出结果，等号右边就是该函数的展开式，展开式就是函数的内容，定义函数的好处是简化式子，可以设x为一个具体的数字，把这个具体的数字带入到等号右边的`x`中进行计算，通过函数的展开式计算出最终的结果。

> 这个函数展开式里面的e是一个常数，此函数的计算原理后面会说明，也可以自行询问AI，让AI给出更详细的描述。

这个函数在函数图中的坐标如下：

![sigmoid激活函数图](./images/sigmoid-image.png "sigmoid激活函数图")

学会函数定义之后，就可以自定义一个属于自己的函数`f(x)`，或者`f(n, m)`，参数可以自己定义，内容也可以自己定义，比如`sigmoid`函数和`softmax`函数就是创造神经网络的数学家或者计算机科学家们定义的，可以直接拿来用。

#### 4.1.2、微积分

微积分，顾名思义，其核心思想正在于 “微分” 与 “积分” 这两个相辅相成的过程。它体现出一种深刻的分治策略：将复杂整体无限细分，化为可描述的简单局部（微分），再将无数简单局部有序合并，重构为可理解的新整体（积分）。

一个经典的例子是圆形面积公式的推导。我们无法直接计算曲线围成的面积，但可以将其微分：想象把圆分割成无数个极细的等腰三角形。当分割无限细密时，每个三角形的弧边便无限接近于直线。然后通过积分：将这些三角形重新拼合，比如组合成一个近似的平行四边形，之后利用已知的简单图形面积公式，最终推导出圆的面积公式。

> 微积分思想的核心——化整为零，积零为整。

在生活中经常用到微积分相关的知识和思想，在生活中还有其他使用微积分的例子，比如路程公式：速度 * 时间 = 路程，这个公式也使用到了微积分的思想。只不过在现实中，汽车和飞机的速度是不稳定的，可能随着时间越来越快，或者随着时间越来越慢，这种情况会在函数图像中形成一条曲线，但是一般只需要计算出来平均速度，而微积分可以通过对路程公式的微分和积分，计算出某个瞬间的速度（加速度），而不仅仅是总体的平均速度。

路程S = 速度v * 时间t，可以得出一个结论，路程的组成部分是速度和时间，而在微积分中，路程是无数个瞬间所走过的路程组合在一起的，也就是无数个速度碎片通过时间组成路程，这种拆分的过程叫微分，不同的时间下，加速度就不一样，组合起来就叫积分：

> 如果速度是时间的函数 v = v(t)，而s是v对时间的积分，就可以得出下列微积分公式，其中的f是微分符号，表示其中一小部分，d是积分符号，表示整体和全部的意思。

![路程时间速度函数](./images/svt-function.png "路程时间速度函数")

现在已经求出整体，但如果需要通过整体去求这个积分函数的组成部分，可以用`ds/dt = v(t)`，读作s对t的导数等于v(t)，从理解上来看，斜杠是一个除号，而导数就是我要寻找的答案，相当于路程除以速度等于时间，而这里的三个项都是函数，所以叫求导数:

![路程时间速度函数](./images/dsdtvs-function.png "路程时间速度函数")

#### 4.1.3 链式法则

通过看视频中的讲解，以及查询百度，询问AI之后，我终于理解到，原来在实际计算中，一个函数整体可能是多个函数的嵌套组成的，必须对一些复杂的算式进行简化，然后用简单的表达式来表示他们之间的关系。这样做的好处就是方便看，方便推导和求导。通过简化复杂的式子，在推导完成后，再进行详细的展开计算，微积分中的**链式法则**公式，是求导的关键。如果有一个函数`y = f(g(x)) * g(x)`，其中y是x的复合函数，求y对x的导数，可以写为：

![一元函数](./images/one-function.png "一元函数")

已开始感觉这个公式看起来很复杂，但是通过询问AI后，我知道可以用莱布尼茨记法简写来替代函数，设u = g(x)，则y = f(u)，则y对x的导数等于y对u的导数乘以u对x的导数：

![求导公式](./images/dao-function.png "求导公式")

### 4.2 前向传播的过程

神经网络分前向传播和反向传播两个部分，前向传播的目的就是，在没有学习的情况下，根据现有的参数，输出一个猜测结果，然后通过反向传播，对猜测结果进行调整，对偏离正确答案的结果做参数的反向调整。

比如开空调的案例，温度偏离自己感觉舒服的温度，高了就往低调，低了就往高调，直到符合预期。因为输入的参数特别多，输出的结果也多，每一个参数，都有可能影响其他的结果，所以每个输入都会与答案相连接，并且每个输入都有不同的参数，根据不同的参数去调整最终的影响程度，但是需要一些兜底的参数，让答案不至于大或者小的离谱，这就是偏置参数的作用。

想象自己觉得有点热，于是去开空调——但你不知道多少度最合适，只能先试一次。 这个“试一次”的过程，就是前向传播：给神经网络一堆输入（当前环境），让它算出一个输出（建议的空调温度），哪怕它一开始是错的。

我们把这个试调过程，拆解成三步走：

**第一步：采集环境信息（输入层）**

首先需要收集几个数据：

- 室温（比如 32℃）
- 湿度（比如 70%）
- 有没有太阳晒进来（0 = 没有，1 = 有）

这三项，就是神经网络的输入，记作 a = [32, 70, 1]

**第二步：算一个“初步感受”，即中间层计算**

这时候心里面会有一个公式：

~~~
体感温度 ≈ 室温 × 0.8 + 湿度 × 0.5 - 阳光修正 × 2 + 基础偏移 10
~~~

这个公式里的数字（0.8、0.5、-2、10），就是权重`w₁`和偏置`b₁`——它们可能是空调师傅教你的“经验值”，但是可能不准确，需要后面调整，计算如下：

~~~
m = 32×0.8 + 70×0.5 + 1×(-2) + 10 = 25.6 + 35 - 2 + 10 = 68.6
~~~

但 68.6℃ 显然不是真实温度，只是自己主观感受的强度值。太抽象了，得把它“翻译”成人话——这就是激活函数`sigmoid`的任务：

> sigmoid 的作用：把任意数字“压缩”到 0～1 之间，代表“有多热”的主观概率感。 比如 sigmoid(68.6) ≈ 1.0 就是特别热；sigmoid(-10) ≈ 0 就是特别冷。

可以通过公式把结果通过函数计算出来：
~~~
h = sigmoid(m) ≈ 1.0
~~~

这里的`h`就是隐藏层输出，即体感温度强度。

**第三步：决定最终建议温度（输出层）**

如果你发现此时变得“很热”，应该如何调整温度，还需要一个“决策规则”：

> “建议设定温度 = 热感强度 × (-5) + 基准温度 25”

这里的 -5 和 25，就是第二组权重 w₂ 和偏置 b₂，然后可以接着计算：

~~~
z = h × (-5) + 25 = 1.0 × (-5) + 25 = 20
~~~

但 20℃ 只是一个数字，如果是多个空调，比如客厅、卧室、书房，可能会算出 [20, 22, 18]，但是此时并不知道哪个应该优先调整，必须使用`softmax`函数将这些值变成一个概率分布：

~~~
o = softmax([20, 22, 18]) ≈ [0.27, 0.49, 0.24]
~~~

这段代码的意思是，调整客厅的概率最高，为49%，也就是此时的最优解为调整客厅，这样前向传播就完成了。

总的流程就是：

> 环境数据 →（乘权重 + 加偏置）→ 主观感受强度 →（sigmoid 压缩）→ 热感程度 →（再乘权重 + 加偏置）→ 建议温度 →（softmax）→ 推荐操作概率，每一步的“权重”和“偏置”，都是可调的经验参数——它们现在可能不准，但没关系，下一步“反向传播”就是来调它们的。

### 4.3 反向传播的过程

反向传播就是在预测结束后，对神经网络的数据，往正确答案的方向上调整。

前向传播其实是根据以下的顺序进行计算的：
~~~
o = softmax( w₂ · sigmoid( w₁·a + b₁ ) + b₂ )
~~~

以上公式看起来很复杂，但是可以将其转换为以下计算过程：

a → [a·w₁ + b₁] → m → sigmoid → h → [h·w₂ + b₂] → z → softmax(z) → o

> 其中的每一个节点都可以是一个函数，每一个括号也可以是一个函数

一般情况下，不需要主动去推导反向传播的过程，除非是为了了解神经网络的底层原理，以下就是这个神经网络的反向传播的推导过程：

在反向传播中，需要通过链式法则对当前函数的组成部分进行求导，因为函数之间的关系就是组合关系，需要找该函数的组成部分，就需要使用链式法则公式`dy/dx = dy/du * du/dx`进行求导，

求softmax的导数：

> 因为o函数的直接组成部分只有`z`，因为`o`是直接通过`z`函数计算得出的结果，所以首先需要对`z`进行求导，`z`经过`softmax`函数的计算，所以求`z`的导数就是`loss`函数，设真实标签为y，即`dL/dz = (o - y)`。因为整个网络结构都是一个完整的算式，所以可以根据链式法则，找到对应函数的组成部分，比如`h * w₂ + b₂ = z`，z函数的组成部分是`h`和`w`，而此处的`b`在函数图像中用于平移，并不影响函数图像的形状，`b`的输入总是1，所以误差对`b`的导数，就等于传递到该层的误差值本身。

求w₂和b₂的导数：

> 根据`h * w₂ + b₂ = z`可以得出对应的求导表达式`dz/dw₂ = h`，`dz/b₂ = 1`，`dz/dh = w₂`，因为此处局部表达式的`h`和`w₂`前面没有数，所以不再分解。因为要修改w₂，可以得出`dL/dw₂ = dL/dz * dz/dw₂ * 1`，但是一般不写1，所以简化为`dL/dw₂ = dL/dz * dz/dw₂`，因为`dz/dw₂ = h`，所以简化为`dL/dw₂ = dL/dz * h`，与学习率相乘，再赋以负号表示反方向调节，让`w₂`加上这个数进行调节就完成对`w₂`的调整。
> 
> 修改`b₂`的式子为`dL/db₂ = dL/dz * dz/db₂`，因为`dz/b₂ = 1`，所以`dL/db₂ = dL/dz`，与学习率相乘，再赋以负号表示反方向调节，让`b₂`加上这个数进行调节就完成对b₂的调整。

求输入层到隐藏层中w₁和b₁的导数：

> 对`sigmoid`求导得`dh/dm = h*(1 - h)`，根据`a * w₁ + b₁ = m`，得`dm/dw₁ = a`, `dm/b₁ = 1`，需要修改w₁，所以求`dL/dw₁：dL/dw₁ = dL/dh * dh/dm * dm/dw₁`;
> 
> 求`dL/db₁`: `dL/db₁ = dL/dh * dh/dm`，同样将结果与学习率相乘，再赋以负号调节即可。

## 五、总结与学习感悟

首先感谢[【【AI入门】【用Excel实现神经网络】】](https://b23.tv/yhO6559)视频给我的提示和启发，此视频以及相关Excel文件，降低了我理解神经网络的门槛，作者老师也给了我帮助，在此感激不尽。

通过这次的学习，我发现学习机器学习的困难点，并非知识本身，而是难以找到一个适合自己的教程，从零到一的过程非常艰难，只要跨过从零到一的过程，往后的路就会平坦很多。我不仅学会看各种公式，我还会推导普通三层网咯的反向传播的过程，我还发现，微积分的求导公式其实可以通过百度或者AI查询，帮助我推导，因为不用考试，所以我只需要关注我需要使用的那部分知识，不用分散注意力到其他地方，这也是学习进展势如破竹的原因。


